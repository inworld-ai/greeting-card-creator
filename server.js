import express from 'express'
import cors from 'cors'
import dotenv from 'dotenv'
import { createGraph, createTTSOnlyGraph, createTextOnlyGraph, createYearInReviewGraph, createWishListGraph } from './graph.js'
import { createRequire } from 'module'
const require = createRequire(import.meta.url)
const wavEncoder = require('wav-encoder')

dotenv.config()

const app = express()
const PORT = process.env.PORT || 3001

// Middleware
app.use(cors())
app.use(express.json({ limit: '10mb' })) // Increase limit for audio uploads
app.use(express.urlencoded({ extended: true, limit: '10mb' }))

// In-memory storage for shared stories (in production, use a database)
const sharedStories = new Map()

// Inworld Voice Clone API endpoint
app.post('/api/clone-voice', async (req, res) => {
  try {
    const { audioData, displayName, transcription, langCode = 'EN_US' } = req.body

    if (!audioData) {
      return res.status(400).json({ 
        error: 'Missing required field: audioData (base64-encoded audio)' 
      })
    }

    if (!displayName || !displayName.trim()) {
      return res.status(400).json({ 
        error: 'Missing required field: displayName' 
      })
    }

    // Use Portal API key for voice cloning, or fall back to standard API key if Portal key not set
    // If Unleash flags are enabled for the workspace, the standard API key may work
    const portalApiKey = process.env.INWORLD_PORTAL_API_KEY || process.env.INWORLD_API_KEY
    if (!portalApiKey) {
      return res.status(500).json({ 
        error: 'Server configuration error: INWORLD_PORTAL_API_KEY or INWORLD_API_KEY not set. Voice cloning requires an API key with Portal access.' 
      })
    }
    
    const usingStandardKey = !process.env.INWORLD_PORTAL_API_KEY && process.env.INWORLD_API_KEY
    if (usingStandardKey) {
      console.log('âš ï¸ Using INWORLD_API_KEY for voice cloning (Portal key not set). Ensure Unleash flags are enabled for Portal API access.')
    }

    // Use the christmas_story_generator workspace
    const workspace = 'christmas_story_generator'
    const parent = `workspaces/${workspace}`

    console.log(`ðŸŽ¤ Voice clone request - Display name: "${displayName}", Lang: ${langCode}, Workspace: ${workspace}`)

    // Call Inworld Voice Clone API
    const cloneResponse = await fetch(`https://api.inworld.ai/voices/v1/${parent}/voices:clone`, {
      method: 'POST',
      headers: {
        'Authorization': `Basic ${portalApiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        displayName: displayName.trim(),
        langCode: langCode,
        voiceSamples: [
          {
            audioData: audioData, // Already base64-encoded from frontend
            transcription: transcription || undefined,
          }
        ],
        audioProcessingConfig: {
          removeBackgroundNoise: false,
        },
      }),
    })

    if (!cloneResponse.ok) {
      const errorData = await cloneResponse.json().catch(() => ({}))
      const errorMessage = errorData.error?.message || errorData.message || `HTTP ${cloneResponse.status}: ${cloneResponse.statusText}`
      
      console.error(`âŒ Voice clone failed: ${errorMessage}`)
      
      if (cloneResponse.status === 401) {
        return res.status(401).json({ 
          error: 'Invalid Portal API key. Please check INWORLD_PORTAL_API_KEY in server configuration.' 
        })
      } else if (cloneResponse.status === 403) {
        return res.status(403).json({ 
          error: 'Access denied. Voice cloning requires preview access. Please contact support@inworld.ai.' 
        })
      } else {
        return res.status(cloneResponse.status).json({ 
          error: `Voice cloning failed: ${errorMessage}` 
        })
      }
    }

    const cloneData = await cloneResponse.json()
    
    console.log(`âœ… Voice cloned successfully - Voice ID: ${cloneData.voice?.voiceId}, Name: ${cloneData.voice?.name}`)

    // Return the voiceId and validation results
    res.json({
      voiceId: cloneData.voice?.voiceId,
      voiceName: cloneData.voice?.name,
      displayName: cloneData.voice?.displayName,
      warnings: cloneData.audioSamplesValidated?.[0]?.warnings || [],
      errors: cloneData.audioSamplesValidated?.[0]?.errors || [],
      transcription: cloneData.audioSamplesValidated?.[0]?.transcription,
    })

  } catch (error) {
    console.error('âŒ Error cloning voice:', error)
    res.status(500).json({ 
      error: `Server error: ${error.message || 'Failed to clone voice'}. Make sure the backend server is running and INWORLD_PORTAL_API_KEY is set.` 
    })
  }
})

// Inworld TTS endpoint using Runtime (for progressive chunks)
app.post('/api/tts', async (req, res) => {
  // Declare variables outside try block so they're accessible in catch
  let selectedVoiceId = null
  
  try {
    const { text, voiceId, apiKey } = req.body

    if (!text) {
      return res.status(400).json({ 
        error: 'Missing required field: text' 
      })
    }

    // Use custom API key if provided, otherwise use server's default
    const selectedApiKey = apiKey || process.env.INWORLD_API_KEY
    if (!selectedApiKey) {
      return res.status(500).json({ 
        error: 'Server configuration error: INWORLD_API_KEY not set and no custom API key provided' 
      })
    }

    // Use voice ID from request, env variable, or default to female voice
    selectedVoiceId = voiceId || process.env.INWORLD_VOICE_ID || 'Wendy'

    // Log the request details for debugging
    const ttsStartTime = Date.now()
    console.log(`ðŸŽµ Runtime TTS request - Request voiceId: "${voiceId}", Selected: "${selectedVoiceId}", Text length: ${text.length}, Using ${apiKey ? 'custom' : 'default'} API key`)
    console.log(`ðŸŽµ Voice ID details - Full: "${selectedVoiceId}", Length: ${selectedVoiceId.length}`)

    // Create TTS-only graph with the selected voice ID (Text â†’ TTS, no TextChunking for speed)
    const graphCreateStart = Date.now()
    const graph = createTTSOnlyGraph(selectedApiKey, selectedVoiceId)
    console.log(`â±ï¸ TTS Graph created in ${Date.now() - graphCreateStart}ms`)

    // Execute the Runtime graph with text input
    const graphStartTime = Date.now()
    const { outputStream } = await graph.start(text)
    console.log(`â±ï¸ TTS Graph started in ${Date.now() - graphStartTime}ms`)

    // Progressive WAV chunking - encode and stream chunks as they're ready
    const SAMPLE_RATE = 24000 // Audio sample rate from Inworld
    const CHUNK_DURATION_SECONDS = 5.0 // Encode chunks every 5 seconds (reduced chunk count, less mid-sentence cuts)
    const SAMPLES_PER_CHUNK = Math.floor(SAMPLE_RATE * CHUNK_DURATION_SECONDS)
    
    res.setHeader('Content-Type', 'application/json') // Send JSON array of WAV chunks
    res.setHeader('Transfer-Encoding', 'chunked')
    
    let currentSamples = []
    let totalSamples = 0
    let firstChunkTime = null
    let chunkIndex = 0
    
    // Helper to encode and send a WAV chunk
    const encodeAndSendChunk = async (samples) => {
      if (samples.length === 0) return
      
      // Calculate total number of samples first
      const totalSamples = samples.reduce((sum, arr) => sum + arr.length, 0)
      if (totalSamples === 0) return
      
      // Create combined array with correct size
      const combinedSamples = new Float32Array(totalSamples)
      let offset = 0
      for (const sampleArray of samples) {
        combinedSamples.set(sampleArray, offset)
        offset += sampleArray.length
      }
      
      const wavData = {
        sampleRate: SAMPLE_RATE,
        channelData: [combinedSamples]
      }
      
      const wavBuffer = await wavEncoder.encode(wavData)
      const buffer = Buffer.from(wavBuffer)
      
      // Send chunk as base64-encoded WAV
      const chunkData = {
        index: chunkIndex++,
        data: buffer.toString('base64'),
        samples: combinedSamples.length
      }
      
      res.write(JSON.stringify(chunkData) + '\n') // Newline-delimited JSON
      console.log(`ðŸŽµ Sent WAV chunk ${chunkData.index}: ${buffer.length} bytes (${combinedSamples.length} samples, ~${(combinedSamples.length / SAMPLE_RATE).toFixed(2)}s)`)
    }

    // Stream audio chunks as they arrive
    let done = false
    while (!done) {
      const result = await outputStream.next()
      
      await result.processResponse({
        TTSOutputStream: async (ttsStream) => {
          // TTS node outputs TTSOutputStream with audio.data (base64 encoded)
          
          for await (const chunk of ttsStream) {
            if (chunk.audio?.data) {
              // Audio data is base64-encoded PCM float32 samples
              const base64Data = chunk.audio.data
              const audioBuffer = Buffer.from(base64Data, 'base64')
              
              if (audioBuffer.length > 0) {
                if (firstChunkTime === null) {
                  firstChunkTime = Date.now()
                  console.log(`â±ï¸ First audio chunk received in ${firstChunkTime - ttsStartTime}ms`)
                }
                
                // Convert buffer to Float32Array (4 bytes per float32 sample)
                const sampleCount = Math.floor(audioBuffer.length / 4)
                if (sampleCount > 0) {
                  // Create a properly aligned Float32Array
                  const float32Samples = new Float32Array(sampleCount)
                  const dataView = new DataView(audioBuffer.buffer, audioBuffer.byteOffset, sampleCount * 4)
                  for (let i = 0; i < sampleCount; i++) {
                    float32Samples[i] = dataView.getFloat32(i * 4, true) // little-endian
                  }
                  
                  // Add to current chunk
                  currentSamples.push(float32Samples)
                  totalSamples += float32Samples.length
                  
                  // Check if we have enough samples for a chunk
                  const currentChunkSamples = currentSamples.reduce((sum, arr) => sum + arr.length, 0)
                  if (currentChunkSamples >= SAMPLES_PER_CHUNK) {
                    // Encode and send this chunk
                    await encodeAndSendChunk(currentSamples)
                    currentSamples = []
                  }
                }
              }
            }
            if (chunk.text) {
              // Log text for debugging (optional)
              console.log(`ðŸ“ TTS text: ${chunk.text.substring(0, 50)}...`)
            }
          }
          
          if (firstChunkTime) {
            console.log(`â±ï¸ Total TTS time: ${Date.now() - ttsStartTime}ms, First chunk: ${firstChunkTime - ttsStartTime}ms`)
          }
        },
        AudioStream: async (audioStream) => {
          // Fallback: handle AudioStream if TTSOutputStream doesn't work
          for await (const audioChunk of audioStream) {
            if (audioChunk.data) {
              const audioData = audioChunk.data
              let audioBuffer
              
              if (Array.isArray(audioData)) {
                audioBuffer = Buffer.from(audioData)
              } else if (typeof audioData === 'string') {
                // If it's base64 encoded
                audioBuffer = Buffer.from(audioData, 'base64')
              } else if (Buffer.isBuffer(audioData)) {
                audioBuffer = audioData
              } else {
                // Try to convert to buffer
                audioBuffer = Buffer.from(audioData)
              }
              
              if (audioBuffer && audioBuffer.length > 0) {
                res.write(audioBuffer)
                console.log(`ðŸŽµ Sent audio chunk (AudioStream): ${audioBuffer.length} bytes`)
              }
            }
          }
        },
        TextStream: async (textStream) => {
          // Text chunks from TextChunkingNode (shouldn't happen in TTS-only, but handle it)
          console.log('ðŸ“ Received text stream in TTS endpoint (unexpected)')
          for await (const textChunk of textStream) {
            console.log('Text chunk:', textChunk.text?.substring(0, 50))
          }
        },
        default: (data) => {
          // Handle other data types
          console.log('Received non-audio data:', typeof data, Object.keys(data || {}))
        },
      })

      done = result.done
    }

    // Clean up
    await graph.stop()
    
    // Send any remaining samples as final chunk
    if (currentSamples.length > 0) {
      await encodeAndSendChunk(currentSamples)
    }
    
    // Send end marker
    res.write(JSON.stringify({ index: -1, end: true }) + '\n')
    res.end()
    
    console.log(`âœ… Sent ${chunkIndex} WAV chunks (${totalSamples} total samples, ~${(totalSamples / SAMPLE_RATE).toFixed(2)}s)`)

  } catch (error) {
    console.error('âŒ Error generating TTS with Runtime:', error)
    console.error('Error stack:', error.stack)
    console.error('Error details:', {
      message: error.message,
      name: error.name,
      cause: error.cause,
    })
    
    const voiceIdUsed = selectedVoiceId || 'unknown'
    
    if (!res.headersSent) {
      // Check for specific error types
      if (error.message?.includes('Unknown voice') || error.message?.includes('voice') || error.message?.includes('Voice')) {
        res.status(400).json({ 
          error: `Invalid request: Voice not found: ${error.message}. Please verify that: 1) Your API key has access to this voice, 2) The voice ID is correct, and 3) The voice exists in your Inworld workspace. Voice ID used: "${voiceIdUsed}". This may indicate that custom voice clones may not be available through Inworld Runtime API. Try using a standard Inworld voice (like "Alex") instead of a custom clone.`
        })
      } else if (error.message?.includes('API key') || error.message?.includes('authentication') || error.message?.includes('401')) {
        res.status(401).json({ 
          error: 'Invalid API key. Please check your INWORLD_API_KEY in the server .env file or verify your custom API key is correct.'
        })
      } else {
        res.status(500).json({ 
          error: `Server error: ${error.message || 'Failed to generate TTS'}. Make sure the backend server is running and INWORLD_API_KEY is set.`
        })
      }
    }
  }
})

// Story generation endpoint using Inworld Runtime
app.post('/api/generate-story', async (req, res) => {
  console.log('\n\nðŸ“– ==========================================')
  console.log('ðŸ“– STORY GENERATION ENDPOINT CALLED (Inworld Runtime)')
  console.log('ðŸ“– ==========================================')
  
  try {
    const { storyType, childName, apiKey } = req.body

    if (!storyType || !childName) {
      return res.status(400).json({ 
        error: 'Missing required fields: storyType and childName' 
      })
    }

    // Use custom API key if provided, otherwise use server's default
    const selectedApiKey = apiKey || process.env.INWORLD_API_KEY
    if (!selectedApiKey) {
      return res.status(500).json({ 
        error: 'Server configuration error: INWORLD_API_KEY not set and no custom API key provided' 
      })
    }

    if (!process.env.GOOGLE_API_KEY) {
      return res.status(500).json({ 
        error: 'Server configuration error: GOOGLE_API_KEY not set' 
      })
    }

    const startTime = Date.now()
    console.log(`ðŸ“– Generating story for "${childName}" about "${storyType}"`)
    console.log(`ðŸ“– Using ${apiKey ? 'custom' : 'default'} API key`)

    // Create text-only graph (LLM only, no TTS)
    const graphCreateTime = Date.now()
    const graph = createTextOnlyGraph(selectedApiKey)
    console.log(`â±ï¸ Graph created in ${Date.now() - graphCreateTime}ms`)

    // Execute the Inworld Runtime graph
    const { outputStream } = await graph.start({
      childName,
      storyType,
    })
    const graphStartTime = Date.now()
    console.log(`â±ï¸ Graph started in ${graphStartTime - graphCreateTime}ms`)

    // Always stream for faster TTS start - set headers immediately
    res.setHeader('Content-Type', 'application/json')
    res.setHeader('Transfer-Encoding', 'chunked')
    
    let storyText = ''
    let firstChunkSent = false
    let done = false

    // Iterate through execution results using the proper pattern
    while (!done) {
      const result = await outputStream.next()
      
      await result.processResponse({
        ContentStream: async (contentStream) => {
          // Process LLM streaming content
          for await (const chunk of contentStream) {
            if (chunk.text) {
              storyText += chunk.text
              console.log(`ðŸ“ Received LLM text chunk: ${chunk.text.substring(0, 50)}...`)
              
              // Send first chunk immediately when we have enough text (first sentence or ~100 chars)
              // This allows TTS to start while the rest of the story is still being generated
              if (!firstChunkSent && storyText.length >= 100) {
                // Try to find a sentence boundary
                const sentenceEnd = storyText.search(/[.!?]\s+/)
                const firstChunk = sentenceEnd > 0 && sentenceEnd < storyText.length * 0.6 
                  ? storyText.substring(0, sentenceEnd + 1).trim()
                  : storyText.substring(0, Math.min(200, storyText.length)).trim()
                
                // Only send if we have a meaningful chunk (at least 80 chars - lower threshold for faster start)
                if (firstChunk.length >= 80) {
                  res.write(JSON.stringify({ 
                    chunkIndex: 0, 
                    text: firstChunk, 
                    isFirst: true,
                    isComplete: false 
                  }) + '\n')
                  firstChunkSent = true
                  console.log(`ðŸ“– Sent first chunk from ContentStream (${firstChunk.length} chars) for early TTS start`)
                }
              }
            }
          }
        },
        TextStream: async (textStream) => {
          // Text chunks from TextChunkingNode
          for await (const textChunk of textStream) {
            if (textChunk.text) {
              storyText += textChunk.text
              console.log(`ðŸ“ Received text chunk: ${textChunk.text.substring(0, 50)}...`)
            }
          }
        },
        string: (text) => {
          // Final aggregated text - might come all at once
          storyText += text
          console.log(`ðŸ“ Received aggregated text: ${text.substring(0, 50)}...`)
          
          // If we haven't sent first chunk yet and we have enough text, send it now
          if (!firstChunkSent && storyText.length >= 100) {
            const sentenceEnd = storyText.search(/[.!?]\s+/)
            const firstChunk = sentenceEnd > 0 && sentenceEnd < storyText.length * 0.6 
              ? storyText.substring(0, sentenceEnd + 1).trim()
              : storyText.substring(0, Math.min(200, storyText.length)).trim()
            
            if (firstChunk.length >= 80) {
              res.write(JSON.stringify({ 
                chunkIndex: 0, 
                text: firstChunk, 
                isFirst: true,
                isComplete: false 
              }) + '\n')
              firstChunkSent = true
              console.log(`ðŸ“– Sent first chunk from aggregated text (${firstChunk.length} chars) for early TTS start`)
            }
          }
        },
        AudioStream: async (audioStream) => {
          // Audio chunks - shouldn't happen in text-only graph, but handle it
          console.log('ðŸŽµ Received audio chunk (unexpected in text-only graph)')
        },
        default: (data) => {
          // Handle other data types
          if (data?.text) {
            storyText += data.text
            console.log(`ðŸ“ Received text: ${data.text.substring(0, 50)}...`)
          } else {
            console.log('Received unknown data type:', typeof data, Object.keys(data || {}))
          }
        },
      })

      done = result.done
    }

    // Clean up
    await graph.stop()

    if (!storyText || storyText.trim().length === 0) {
      console.error('âŒ No story text generated from Runtime')
      return res.status(500).json({ error: 'No story generated from Runtime' })
    }

    const endTime = Date.now()
    const totalTime = endTime - startTime
    console.log(`âœ… Generated story (first 200 chars): ${storyText.substring(0, 200)}...`)
    console.log(`Story mentions "${storyType}": ${storyText.toLowerCase().includes(storyType.toLowerCase())}`)
    console.log(`â±ï¸ Total story generation time: ${totalTime}ms (${(totalTime / 1000).toFixed(2)}s)`)

    // Always send final chunk (we're always streaming now)
    if (firstChunkSent) {
      // Send remaining text (everything after the first chunk) as final chunk
      // Find where the first chunk ended in the full story
      const firstChunkEnd = storyText.search(/[.!?]\s+/)
      const firstChunkLength = firstChunkEnd > 0 && firstChunkEnd < storyText.length * 0.6 
        ? firstChunkEnd + 1
        : Math.min(200, storyText.length)
      
      const remainingText = storyText.substring(firstChunkLength).trim()
      if (remainingText) {
        res.write(JSON.stringify({ 
          chunkIndex: 1, 
          text: storyText.trim(), // Send full story for second chunk (frontend will handle splitting)
          isFirst: false,
          isComplete: true 
        }) + '\n')
      } else {
        // No remaining text, just mark as complete
        res.write(JSON.stringify({ 
          chunkIndex: 1, 
          text: '', 
          isFirst: false,
          isComplete: true 
        }) + '\n')
      }
    } else {
      // If we never sent first chunk (story was too short), send full story but mark as first chunk
      // Try to send a partial chunk anyway for early TTS start
      if (storyText.length >= 100) {
        const sentenceEnd = storyText.search(/[.!?]\s+/)
        const firstChunk = sentenceEnd > 0 && sentenceEnd < storyText.length * 0.6 
          ? storyText.substring(0, sentenceEnd + 1).trim()
          : storyText.substring(0, Math.min(200, storyText.length)).trim()
        
        if (firstChunk.length >= 80) {
          res.write(JSON.stringify({ 
            chunkIndex: 0, 
            text: firstChunk, 
            isFirst: true,
            isComplete: false 
          }) + '\n')
          firstChunkSent = true
          // Then send full story as second chunk
          res.write(JSON.stringify({ 
            chunkIndex: 1, 
            text: storyText.trim(), 
            isFirst: false,
            isComplete: true 
          }) + '\n')
        } else {
          // Too short, just send full story
          res.write(JSON.stringify({ 
            chunkIndex: 0, 
            text: storyText.trim(), 
            isFirst: true,
            isComplete: true 
          }) + '\n')
        }
      } else {
        // Story too short, just send it
        res.write(JSON.stringify({ 
          chunkIndex: 0, 
          text: storyText.trim(), 
          isFirst: true,
          isComplete: true 
        }) + '\n')
      }
    }
    res.end()
  } catch (error) {
    console.error('âŒ Error generating story with Inworld Runtime:', error)
    
    let statusCode = 500
    let errorMessage = 'Failed to generate story'

    // Check for authorization/authentication errors
    if (error.message?.includes('Invalid authorization') || 
        error.message?.includes('authorization credentials') ||
        error.message?.includes('API key') || 
        error.message?.includes('authentication') ||
        error.message?.includes('401')) {
      statusCode = 401
      // Check if this is a custom API key (from request body)
      if (req.body?.apiKey) {
        errorMessage = 'Invalid authorization credentials. Please double-check your Inworld API Key. Make sure it\'s the correct Base64-encoded key copied from your Inworld workspace (API Keys â†’ Copy the "Basic (Base64) key").'
      } else {
        errorMessage = 'Invalid API key. Please check your GOOGLE_API_KEY and INWORLD_API_KEY in the .env file.'
      }
    } else if (error.message?.includes('rate limit') || error.message?.includes('quota')) {
      statusCode = 429
      errorMessage = 'Rate limit exceeded. Please wait a moment and try again.'
    } else if (error.message) {
      errorMessage = error.message
    }

    res.status(statusCode).json({ error: errorMessage })
  }
})

// Conversational question endpoint - generates a conversational question using LLM
app.post('/api/conversational-question', async (req, res) => {
  console.log('\n\nðŸ’¬ ==========================================')
  console.log('ðŸ’¬ CONVERSATIONAL QUESTION ENDPOINT CALLED')
  console.log('ðŸ’¬ ==========================================')
  
  try {
    const { questionPrompt, conversationHistory } = req.body

    if (!questionPrompt) {
      return res.status(400).json({ 
        error: 'Missing required field: questionPrompt' 
      })
    }

    if (!process.env.INWORLD_API_KEY) {
      return res.status(500).json({ 
        error: 'Server configuration error: INWORLD_API_KEY not set' 
      })
    }

    if (!process.env.GOOGLE_API_KEY) {
      return res.status(500).json({ 
        error: 'Server configuration error: GOOGLE_API_KEY not set' 
      })
    }

    const { createTextOnlyGraph } = require('./graph.js')
    const graph = createTextOnlyGraph(process.env.INWORLD_API_KEY)

    // Build conversation context
    let conversationContext = ''
    if (conversationHistory && conversationHistory.length > 0) {
      conversationContext = '\n\nPrevious conversation:\n'
      conversationHistory.forEach((item, index) => {
        conversationContext += `${index + 1}. Question: ${item.question}\n   Answer: ${item.answer}\n\n`
      })
    }

    const userPrompt = `You are a warm, friendly AI assistant conducting a conversational interview. ${questionPrompt}

${conversationContext}

Generate a single, natural, conversational question that asks about this topic. Make it sound like a friendly conversation, not a formal interview. Keep it to one sentence. Do not include any prefixes like "Question:" or numbering. Just return the question itself.`

    const { outputStream } = await graph.start(userPrompt)

    let questionText = ''
    let done = false

    while (!done) {
      const result = await outputStream.next()
      
      await result.processResponse({
        ContentStream: async (contentStream) => {
          for await (const chunk of contentStream) {
            if (chunk.text) {
              questionText += chunk.text
            }
          }
        },
        default: (data) => {
          if (data?.text) {
            questionText += data.text
          }
        },
      })

      done = result.done
    }

    await graph.stop()

    if (!questionText || questionText.trim().length === 0) {
      return res.status(500).json({ error: 'No question generated' })
    }

    // Clean up the question text (remove any prefixes, extra whitespace)
    const cleanQuestion = questionText.trim().replace(/^(Question:\s*|Q:\s*|\d+\.\s*)/i, '').trim()

    console.log(`âœ… Generated conversational question: ${cleanQuestion}`)

    return res.status(200).json({ question: cleanQuestion })
  } catch (error) {
    console.error('âŒ Error generating conversational question:', error)
    const statusCode = error.statusCode || 500
    let errorMessage = 'Internal server error'
    
    if (error.message) {
      errorMessage = error.message
    }

    res.status(statusCode).json({ error: errorMessage })
  }
})

// Conversational chat endpoint - maintains conversation state and handles back-and-forth dialogue
app.post('/api/conversational-chat', async (req, res) => {
  console.log('\n\nðŸ’¬ ==========================================')
  console.log('ðŸ’¬ CONVERSATIONAL CHAT ENDPOINT CALLED')
  console.log('ðŸ’¬ ==========================================')
  
  try {
    const { 
      experienceType, 
      userMessage, 
      conversationHistory = [],
      answeredQuestions = {}
    } = req.body

    if (!experienceType) {
      return res.status(400).json({ 
        error: 'Missing required field: experienceType' 
      })
    }

    if (!process.env.INWORLD_API_KEY) {
      return res.status(500).json({ 
        error: 'Server configuration error: INWORLD_API_KEY not set' 
      })
    }

    if (!process.env.GOOGLE_API_KEY) {
      return res.status(500).json({ 
        error: 'Server configuration error: GOOGLE_API_KEY not set' 
      })
    }

    // Create a custom graph for conversational chat
    const {
      RemoteLLMChatNode,
      SequentialGraphBuilder,
    } = require('@inworld/runtime/graph');

    const graphBuilder = new SequentialGraphBuilder({
      id: 'conversational-chat',
      apiKey: process.env.INWORLD_API_KEY,
      enableRemoteConfig: false,
      nodes: [
        new RemoteLLMChatNode({
          provider: 'google',
          modelName: 'gemini-2.5-flash-lite',
          stream: true,
          messageTemplates: [
            {
              role: 'system',
              content: {
                type: 'template',
                template: '{{systemPrompt}}',
              },
            },
            {
              role: 'user',
              content: {
                type: 'template',
                template: '{{userMessage}}',
              },
            },
          ],
        }),
      ],
    });

    const graph = graphBuilder.build()

    // Define the three questions based on experience type
    const questions = experienceType === 'year-review'
      ? [
          { key: 'favoriteMemory', question: "What was your favorite memory or adventure from 2025?" },
          { key: 'newThing', question: "What's something new you tried or learned in 2025?" },
          { key: 'lookingForward', question: "What are you most looking forward to or hoping for in 2026?" }
        ]
      : [
          { key: 'dreamGift', question: "What's the one gift you've been thinking about all year?" },
          { key: 'experience', question: "Is there something you'd love to experience rather than receive? (like a trip, concert, or special dinner)" },
          { key: 'practicalNeed', question: "What's something practical you actually need but wouldn't buy for yourself?" }
        ]

    // Build conversation context
    let conversationContext = ''
    if (conversationHistory && conversationHistory.length > 0) {
      conversationContext = '\n\nConversation so far:\n'
      conversationHistory.forEach((item, index) => {
        conversationContext += `${index + 1}. ${item.role === 'assistant' ? 'You' : 'User'}: ${item.content}\n`
      })
    }

    // Build answered questions context
    let answeredContext = ''
    const answeredKeys = Object.keys(answeredQuestions)
    if (answeredKeys.length > 0) {
      answeredContext = '\n\nQuestions already answered:\n'
      answeredKeys.forEach(key => {
        const question = questions.find(q => q.key === key)
        if (question) {
          answeredContext += `- ${question.question}: ${answeredQuestions[key]}\n`
        }
      })
    }

    // Determine which questions still need to be answered
    const remainingQuestions = questions.filter(q => !answeredQuestions[q.key])
    const nextQuestion = remainingQuestions[0]

    // Determine the experience name for the prompt
    const experienceName = experienceType === 'year-review' ? 'Year In Review' : 'Christmas Wish List'

    // Check which questions have already been ASKED (not just answered) in conversation history
    const askedQuestions = new Set()
    if (conversationHistory && conversationHistory.length > 0) {
      questions.forEach(q => {
        // Check if this question appears in any assistant message
        const questionAsked = conversationHistory.some(msg => {
          if (msg.role === 'assistant') {
            const msgLower = msg.content.toLowerCase()
            const questionStart = q.question.substring(0, 30).toLowerCase()
            // Check if the question text appears in the assistant message
            if (msgLower.includes(questionStart)) {
              // Make sure it's not matching another question
              const otherQuestions = questions.filter(otherQ => otherQ.key !== q.key)
              return !otherQuestions.some(otherQ => msgLower.includes(otherQ.question.substring(0, 20).toLowerCase()))
            }
          }
          return false
        })
        if (questionAsked) {
          askedQuestions.add(q.key)
        }
      })
    }

    // Build explicit list of questions that have been asked
    const askedQuestionsList = Array.from(askedQuestions).map(key => {
      const q = questions.find(q => q.key === key)
      return q ? q.question : null
    }).filter(Boolean)

    // Build system prompt
    const systemPrompt = `You are Olivia, a warm, friendly, and conversational AI assistant conducting a personalized interview. You're having a natural, flowing conversation with someone, not conducting a formal questionnaire.

Your goal is to gather information about three topics through natural conversation:
${questions.map((q, i) => `${i + 1}. ${q.question}`).join('\n')}

CRITICAL RULES - FOLLOW THESE STRICTLY - NO EXCEPTIONS:
1. You may ask ONE follow-up question per topic MAXIMUM - no exceptions
2. After asking the initial question and receiving a response, you may ask ONE follow-up if needed
3. After the follow-up response, you MUST immediately move on to the next topic - do NOT ask another follow-up
4. If you've already asked a follow-up about a topic, you MUST move to the next topic in your next response
5. Do NOT ask multiple follow-ups about the same topic - this is strictly forbidden
6. DO NOT REPEAT QUESTIONS - If a question has already been asked in the conversation, you MUST NOT ask it again
7. If you see a question in the "Questions already asked" list below, you MUST NOT ask it again - move to the next question instead
8. If the user gives a detailed or complete answer (especially if they selected a preset option), you may skip the follow-up and move directly to the next question

${askedQuestionsList.length > 0 ? `\nQUESTIONS ALREADY ASKED (DO NOT ASK THESE AGAIN):\n${askedQuestionsList.map((q, i) => `${i + 1}. ${q}`).join('\n')}\n` : ''}

IMPORTANT GUIDELINES:
- Be warm, friendly, and conversational - like talking to a friend
- Ask questions naturally and follow up if you need clarification (but only ONE follow-up per topic)
- If the user gives a detailed, complete answer (especially if it mentions specific items like "laptop", "tablet", "camera", "getaway", "concert", "shoes", "coffee maker", etc.), treat it as a complete answer and SKIP the follow-up - move directly to the next question
- If the user gives a brief answer, you may ask ONE gentle follow-up question to get more detail, then IMMEDIATELY move on
- If the user asks a question, answer it naturally and then continue with the interview
- Once you have asked the initial question and at most one follow-up about a topic, you MUST smoothly transition to the next question
- Don't rush - take your time to have a real conversation
- Keep your responses concise (1-2 sentences typically)
- If this is the first message and no conversation has started, begin by introducing yourself warmly and asking the first question
- If all three questions have been answered, thank them warmly and say: "Thank you so much for sharing! I'll take your answers and create your ${experienceName} now." Make sure to include the exact phrase "I'll take your answers and create your ${experienceName} now" in your closing message.

${answeredContext}

${nextQuestion ? `\nNEXT QUESTION TO ASK: "${nextQuestion.question}"\n\nIMPORTANT: This is the question you should ask NEXT. You have NOT asked this question yet. If you see this question in the "Questions already asked" list above, DO NOT ask it - there is an error, move to the next topic instead.` : '\nAll questions have been answered. Wrap up the conversation.\n'}

${conversationContext}

FINAL REMINDER: Review the conversation history above. If you have already asked a question (check the "Questions already asked" list), do NOT ask it again. Move on to the next question or wrap up if all questions are answered.`

    // For the first message, use a default greeting prompt
    const userPrompt = userMessage || "Hello, let's start the conversation!"

    console.log(`ðŸ’¬ Starting conversation with userMessage: "${userPrompt}"`)
    console.log(`ðŸ’¬ System prompt length: ${systemPrompt.length}`)
    console.log(`ðŸ’¬ Conversation history length: ${conversationHistory.length}`)

    const { outputStream } = await graph.start({
      systemPrompt: systemPrompt,
      userMessage: userPrompt
    })

    let responseText = ''
    let done = false
    let hasContent = false

    while (!done) {
      const result = await outputStream.next()
      
      await result.processResponse({
        ContentStream: async (contentStream) => {
          hasContent = true
          for await (const chunk of contentStream) {
            if (chunk.text) {
              responseText += chunk.text
              console.log(`ðŸ’¬ Received text chunk: ${chunk.text.substring(0, 50)}...`)
            }
          }
        },
        Content: (content) => {
          hasContent = true
          if (content.content) {
            responseText += content.content
            console.log(`ðŸ’¬ Received content: ${content.content.substring(0, 50)}...`)
          }
        },
        default: (data) => {
          if (data?.text) {
            hasContent = true
            responseText += data.text
            console.log(`ðŸ’¬ Received default text: ${data.text.substring(0, 50)}...`)
          } else if (typeof data === 'string') {
            hasContent = true
            responseText += data
            console.log(`ðŸ’¬ Received string: ${data.substring(0, 50)}...`)
          } else {
            console.log(`ðŸ’¬ Received unknown data type:`, typeof data, data)
          }
        },
        error: (error) => {
          console.error('âŒ Graph error:', error)
          throw error
        }
      })

      done = result.done
    }

    await graph.stop()

    console.log(`ðŸ’¬ Final response text length: ${responseText.length}`)
    console.log(`ðŸ’¬ Has content: ${hasContent}`)

    if (!hasContent || !responseText || responseText.trim().length === 0) {
      console.error('âŒ No response generated from graph')
      return res.status(500).json({ error: 'No response generated from AI' })
    }

    const cleanResponse = responseText.trim()

    // Determine if a question was answered based on the conversation flow
    // More sophisticated detection: check if we have enough information about the current topic
    let detectedAnswer = null
    let detectedQuestionKey = null
    
    if (userMessage && userMessage.trim().length > 10 && nextQuestion) {
      const responseLower = cleanResponse.toLowerCase()
      const userMessageLower = userMessage.toLowerCase()
      
      // Check if AI is asking a follow-up about the SAME topic (not moving on)
      const isFollowUp = responseLower.includes('what was') || 
                        responseLower.includes('tell me more') ||
                        responseLower.includes('can you tell me') ||
                        responseLower.includes('what made') ||
                        responseLower.includes('specific') ||
                        responseLower.includes('particular') ||
                        (responseLower.includes('?') && !responseLower.includes('next') && !responseLower.includes('another'))
      
      // Check if AI is moving to the next topic - be more aggressive in detection
      const movesToNext = responseLower.includes('next') ||
                          responseLower.includes('another') ||
                          responseLower.includes('also') ||
                          responseLower.includes('what\'s something new') ||
                          responseLower.includes('what are you looking forward') ||
                          responseLower.includes('anything new') ||
                          responseLower.includes('something new') ||
                          responseLower.includes('tried or learned') ||
                          responseLower.includes('looking forward') ||
                          responseLower.includes('hoping for') ||
                          responseLower.includes('one gift') ||
                          responseLower.includes('something you\'d love') ||
                          responseLower.includes('something practical') ||
                          (answeredKeys.length > 0 && !isFollowUp && !responseLower.includes('?'))
      
      // Check if AI is acknowledging and wrapping up the current topic
      const acknowledgesAndWraps = (responseLower.includes('thank') || 
                                    responseLower.includes('great') ||
                                    responseLower.includes('wonderful') ||
                                    responseLower.includes('lovely') ||
                                    responseLower.includes('sounds') ||
                                    responseLower.includes('perfect') ||
                                    responseLower.includes('amazing')) &&
                                   !isFollowUp
      
      // Check if user message looks like a preset option (complete, detailed answer)
      const looksLikePreset = userMessage.length > 20 && 
                              (userMessage.includes('laptop') || 
                               userMessage.includes('tablet') ||
                               userMessage.includes('camera') ||
                               userMessage.includes('subscription') ||
                               userMessage.includes('getaway') ||
                               userMessage.includes('concert') ||
                               userMessage.includes('cooking class') ||
                               userMessage.includes('wine tasting') ||
                               userMessage.includes('shoes') ||
                               userMessage.includes('boots') ||
                               userMessage.includes('coffee maker') ||
                               userMessage.includes('appliance') ||
                               userMessage.includes('organizer'))
      
      // Count user responses about the current topic
      // Find when the current question was first asked
      const questionFirstAskedIndex = conversationHistory.findIndex(msg => 
        msg.role === 'assistant' && 
        msg.content.toLowerCase().includes(nextQuestion.question.substring(0, 20).toLowerCase())
      )
      
      let userResponseCount = 0
      if (questionFirstAskedIndex >= 0) {
        // Count user responses after this question was asked, but before next question
        const nextQuestionIndex = conversationHistory.findIndex((msg, idx) => 
          idx > questionFirstAskedIndex &&
          msg.role === 'assistant' &&
          questions.some(q => q.key !== nextQuestion.key && msg.content.toLowerCase().includes(q.question.substring(0, 20).toLowerCase()))
        )
        
        const endIndex = nextQuestionIndex >= 0 ? nextQuestionIndex : conversationHistory.length
        userResponseCount = conversationHistory
          .slice(questionFirstAskedIndex + 1, endIndex)
          .filter(msg => msg.role === 'user').length
      }
      // Add current response
      userResponseCount += 1
      
      console.log(`ðŸ” Detection check: userMessage length=${userMessage.length}, looksLikePreset=${looksLikePreset}, movesToNext=${movesToNext}, isFollowUp=${isFollowUp}, userResponseCount=${userResponseCount}, acknowledgesAndWraps=${acknowledgesAndWraps}`)
      
      // More aggressive detection: if AI moves to next question OR user has given 2+ responses OR it looks like a preset, mark as answered
      if (userMessage.length > 10 && (movesToNext || looksLikePreset || (acknowledgesAndWraps && userResponseCount >= 2))) {
        // Determine which question was actually answered
        // CRITICAL: If Olivia moved to the next question, the answer is for the PREVIOUS question
        // Otherwise, find which question was most recently asked
        let questionToAnswer = nextQuestion // Default fallback
        
        if (movesToNext) {
          // Olivia moved to the next question, so the user's answer was for the PREVIOUS question
          // Find which question Olivia just asked in the CURRENT response
          const matchingNewQuestion = questions.find(q => {
            if (answeredQuestions[q.key]) return false // Already answered
            const questionStart = q.question.substring(0, 30).toLowerCase()
            return cleanResponse.toLowerCase().includes(questionStart)
          })
          
          if (matchingNewQuestion) {
            // Find the question that was asked BEFORE this new one
            // Look through all questions to find which one comes before matchingNewQuestion
            const questionIndex = questions.findIndex(q => q.key === matchingNewQuestion.key)
            if (questionIndex > 0) {
              // The previous question in the list
              questionToAnswer = questions[questionIndex - 1]
              console.log(`ðŸ” AI moved to "${matchingNewQuestion.key}", so answer is for previous question: "${questionToAnswer.key}"`)
            } else {
              // Fallback: look at conversation history to find what was asked before
              questionToAnswer = nextQuestion
              console.log(`ðŸ” AI moved to "${matchingNewQuestion.key}", using nextQuestion as fallback: "${questionToAnswer.key}"`)
            }
          } else {
            // Couldn't find the new question, use nextQuestion
            questionToAnswer = nextQuestion
            console.log(`ðŸ” AI moved to next question but couldn't identify it, using nextQuestion: "${questionToAnswer.key}"`)
          }
        } else {
          // Olivia didn't move to next question, so find which question was most recently asked
          // Check the CURRENT response first (it might contain the question)
          const responseLower = cleanResponse.toLowerCase()
          for (const q of questions) {
            if (answeredQuestions[q.key]) continue // Skip already answered questions
            const questionStart = q.question.substring(0, 30).toLowerCase()
            if (responseLower.includes(questionStart)) {
              // Make sure it doesn't match other questions
              const otherQuestions = questions.filter(otherQ => otherQ.key !== q.key)
              const isOtherQuestion = otherQuestions.some(otherQ => 
                responseLower.includes(otherQ.question.substring(0, 20).toLowerCase())
              )
              if (!isOtherQuestion) {
                questionToAnswer = q
                console.log(`ðŸ” Found question in current response: "${q.key}"`)
                break
              }
            }
          }
          
          // If not found in current response, check conversation history
          if (questionToAnswer === nextQuestion) {
            const recentAssistantMessages = conversationHistory
              .slice()
              .reverse()
              .filter(msg => msg.role === 'assistant')
            
            for (const msg of recentAssistantMessages) {
              const msgLower = msg.content.toLowerCase()
              // Check each question to see if it was asked in this message
              for (const q of questions) {
                if (answeredQuestions[q.key]) continue // Skip already answered questions
                
                const questionStart = q.question.substring(0, 30).toLowerCase()
                if (msgLower.includes(questionStart)) {
                  // Make sure it doesn't match other questions
                  const otherQuestions = questions.filter(otherQ => otherQ.key !== q.key)
                  const isOtherQuestion = otherQuestions.some(otherQ => 
                    msgLower.includes(otherQ.question.substring(0, 20).toLowerCase())
                  )
                  
                  if (!isOtherQuestion) {
                    // This is the question that was most recently asked
                    questionToAnswer = q
                    console.log(`ðŸ” Found most recently asked question in history: "${q.key}"`)
                    break
                  }
                }
              }
              if (questionToAnswer !== nextQuestion) break // Found it, stop looking
            }
          }
        }
        
        // Collect all user responses about this topic
        const topicResponses = []
        // Find when this specific question was first asked (not just any question)
        const specificQuestionIndex = conversationHistory.findIndex(msg => {
          if (msg.role !== 'assistant') return false
          const msgLower = msg.content.toLowerCase()
          const questionStart = questionToAnswer.question.substring(0, 25).toLowerCase()
          if (msgLower.includes(questionStart)) {
            // Make sure it doesn't match other questions
            const otherQuestions = questions.filter(q => q.key !== questionToAnswer.key)
            return !otherQuestions.some(q => msgLower.includes(q.question.substring(0, 20).toLowerCase()))
          }
          return false
        })
        
        if (specificQuestionIndex >= 0) {
          // Find when the next question was asked (to stop collecting responses)
          const nextQuestionIndex = conversationHistory.findIndex((msg, idx) => 
            idx > specificQuestionIndex &&
            msg.role === 'assistant' &&
            questions.some(q => q.key !== questionToAnswer.key && msg.content.toLowerCase().includes(q.question.substring(0, 20).toLowerCase()))
          )
          const endIndex = nextQuestionIndex >= 0 ? nextQuestionIndex : conversationHistory.length
          conversationHistory
            .slice(specificQuestionIndex + 1, endIndex)
            .filter(msg => msg.role === 'user')
            .forEach(msg => topicResponses.push(msg.content))
        }
        topicResponses.push(userMessage)
        
        detectedAnswer = topicResponses.join(' ')
        detectedQuestionKey = questionToAnswer.key
        console.log(`âœ… Detected answer for ${detectedQuestionKey} after ${userResponseCount} responses (movesToNext: ${movesToNext})`)
        console.log(`ðŸ“ Answer text: ${detectedAnswer.substring(0, 100)}...`)
      }
    }

    console.log(`âœ… Generated conversational response: ${cleanResponse.substring(0, 100)}...`)
    if (detectedAnswer) {
      console.log(`âœ… Detected answer for question: ${detectedQuestionKey}`)
    }

    return res.status(200).json({ 
      response: cleanResponse,
      detectedAnswer: detectedAnswer,
      detectedQuestionKey: detectedQuestionKey
    })
  } catch (error) {
    console.error('âŒ Error in conversational chat:', error)
    const statusCode = error.statusCode || 500
    let errorMessage = 'Internal server error'
    
    if (error.message) {
      errorMessage = error.message
    }

    res.status(statusCode).json({ error: errorMessage })
  }
})

// Year in Review generation endpoint
app.post('/api/generate-year-review', async (req, res) => {
  console.log('\n\nðŸ“ ==========================================')
  console.log('ðŸ“ YEAR IN REVIEW GENERATION ENDPOINT CALLED')
  console.log('ðŸ“ ==========================================')
  
  try {
    const { favoriteMemory, newThing, lookingForward, name, apiKey, isCustomVoice } = req.body

    if (!favoriteMemory || !newThing || !lookingForward || !name) {
      return res.status(400).json({ 
        error: 'Missing required fields: favoriteMemory, newThing, lookingForward, name' 
      })
    }

    const selectedApiKey = apiKey || process.env.INWORLD_API_KEY
    if (!selectedApiKey) {
      return res.status(500).json({ 
        error: 'Server configuration error: INWORLD_API_KEY not set and no custom API key provided' 
      })
    }

    if (!process.env.GOOGLE_API_KEY) {
      return res.status(500).json({ 
        error: 'Server configuration error: GOOGLE_API_KEY not set' 
      })
    }

    // Determine if custom voice (first person) or preset voice (third person)
    const useCustomVoice = isCustomVoice === true || isCustomVoice === 'true'
    
    const startTime = Date.now()
    console.log(`ðŸ“ Generating year in review for ${name}`)
    console.log(`ðŸ“ Using ${apiKey ? 'custom' : 'default'} API key`)
    console.log(`ðŸ“ Perspective: ${useCustomVoice ? 'First person (custom voice)' : 'Third person (preset voice)'}`)

    const graph = createYearInReviewGraph(selectedApiKey, useCustomVoice)
    const { outputStream } = await graph.start({
      name,
      favoriteMemory,
      newThing,
      lookingForward,
    })

    res.setHeader('Content-Type', 'application/json')
    res.setHeader('Transfer-Encoding', 'chunked')
    
    let storyText = ''
    let firstChunkSent = false
    let done = false

    while (!done) {
      const result = await outputStream.next()
      
      await result.processResponse({
        ContentStream: async (contentStream) => {
          for await (const chunk of contentStream) {
            if (chunk.text) {
              storyText += chunk.text
              
              if (!firstChunkSent && storyText.length >= 100) {
                const sentenceEnd = storyText.search(/[.!?]\s+/)
                const firstChunk = sentenceEnd > 0 && sentenceEnd < storyText.length * 0.6 
                  ? storyText.substring(0, sentenceEnd + 1).trim()
                  : storyText.substring(0, Math.min(200, storyText.length)).trim()
                
                if (firstChunk.length >= 80) {
                  res.write(JSON.stringify({ 
                    chunkIndex: 0, 
                    text: firstChunk, 
                    isFirst: true,
                    isComplete: false 
                  }) + '\n')
                  firstChunkSent = true
                }
              }
            }
          }
        },
        default: (data) => {
          if (data?.text) {
            storyText += data.text
          }
        },
      })

      done = result.done
    }

    await graph.stop()

    if (!storyText || storyText.trim().length === 0) {
      return res.status(500).json({ error: 'No story generated' })
    }

    if (firstChunkSent) {
      res.write(JSON.stringify({ 
        chunkIndex: 1, 
        text: storyText.trim(),
        isFirst: false,
        isComplete: true 
      }) + '\n')
    } else {
      if (storyText.length >= 100) {
        const sentenceEnd = storyText.search(/[.!?]\s+/)
        const firstChunk = sentenceEnd > 0 && sentenceEnd < storyText.length * 0.6 
          ? storyText.substring(0, sentenceEnd + 1).trim()
          : storyText.substring(0, Math.min(200, storyText.length)).trim()
        
        if (firstChunk.length >= 80) {
          res.write(JSON.stringify({ 
            chunkIndex: 0, 
            text: firstChunk, 
            isFirst: true,
            isComplete: false 
          }) + '\n')
          res.write(JSON.stringify({ 
            chunkIndex: 1, 
            text: storyText.trim(),
            isFirst: false,
            isComplete: true 
          }) + '\n')
        } else {
          res.write(JSON.stringify({ 
            chunkIndex: 0, 
            text: storyText.trim(), 
            isFirst: true,
            isComplete: true 
          }) + '\n')
        }
      } else {
        res.write(JSON.stringify({ 
          chunkIndex: 0, 
          text: storyText.trim(), 
          isFirst: true,
          isComplete: true 
        }) + '\n')
      }
    }
    res.end()
  } catch (error) {
    console.error('âŒ Error generating year in review:', error)
    
    let statusCode = 500
    let errorMessage = 'Failed to generate year in review'

    if (error.message?.includes('Invalid authorization') || 
        error.message?.includes('authorization credentials') ||
        error.message?.includes('API key') || 
        error.message?.includes('authentication') ||
        error.message?.includes('401')) {
      statusCode = 401
      if (req.body?.apiKey) {
        errorMessage = 'Invalid authorization credentials. Please double-check your Inworld API Key.'
      } else {
        errorMessage = 'Invalid API key. Please check your GOOGLE_API_KEY and INWORLD_API_KEY in the .env file.'
      }
    } else if (error.message?.includes('rate limit') || error.message?.includes('quota')) {
      statusCode = 429
      errorMessage = 'Rate limit exceeded. Please wait a moment and try again.'
    } else if (error.message) {
      errorMessage = error.message
    }

    res.status(statusCode).json({ error: errorMessage })
  }
})

// Wish List generation endpoint
app.post('/api/generate-wish-list', async (req, res) => {
  console.log('\n\nðŸŽ ==========================================')
  console.log('ðŸŽ WISH LIST GENERATION ENDPOINT CALLED')
  console.log('ðŸŽ ==========================================')
  
  try {
    const { dreamGift, experience, practicalNeed, name, apiKey, isCustomVoice } = req.body

    if (!dreamGift || !experience || !practicalNeed || !name) {
      return res.status(400).json({ 
        error: 'Missing required fields: dreamGift, experience, practicalNeed, name' 
      })
    }

    const selectedApiKey = apiKey || process.env.INWORLD_API_KEY
    if (!selectedApiKey) {
      return res.status(500).json({ 
        error: 'Server configuration error: INWORLD_API_KEY not set and no custom API key provided' 
      })
    }

    if (!process.env.GOOGLE_API_KEY) {
      return res.status(500).json({ 
        error: 'Server configuration error: GOOGLE_API_KEY not set' 
      })
    }

    // Determine if custom voice (first person) or preset voice (third person)
    const useCustomVoice = isCustomVoice === true || isCustomVoice === 'true'
    
    const startTime = Date.now()
    console.log(`ðŸŽ Generating wish list for ${name}`)
    console.log(`ðŸŽ Using ${apiKey ? 'custom' : 'default'} API key`)
    console.log(`ðŸŽ Perspective: ${useCustomVoice ? 'First person (custom voice)' : 'Third person (preset voice)'}`)

    const graph = createWishListGraph(selectedApiKey, useCustomVoice)
    const { outputStream } = await graph.start({
      name,
      dreamGift,
      experience,
      practicalNeed,
    })

    res.setHeader('Content-Type', 'application/json')
    res.setHeader('Transfer-Encoding', 'chunked')
    
    let listText = ''
    let firstChunkSent = false
    let done = false

    while (!done) {
      const result = await outputStream.next()
      
      await result.processResponse({
        ContentStream: async (contentStream) => {
          for await (const chunk of contentStream) {
            if (chunk.text) {
              listText += chunk.text
              
              if (!firstChunkSent && listText.length >= 100) {
                const sentenceEnd = listText.search(/[.!?]\s+/)
                const firstChunk = sentenceEnd > 0 && sentenceEnd < listText.length * 0.6 
                  ? listText.substring(0, sentenceEnd + 1).trim()
                  : listText.substring(0, Math.min(200, listText.length)).trim()
                
                if (firstChunk.length >= 80) {
                  res.write(JSON.stringify({ 
                    chunkIndex: 0, 
                    text: firstChunk, 
                    isFirst: true,
                    isComplete: false 
                  }) + '\n')
                  firstChunkSent = true
                }
              }
            }
          }
        },
        default: (data) => {
          if (data?.text) {
            listText += data.text
          }
        },
      })

      done = result.done
    }

    await graph.stop()

    if (!listText || listText.trim().length === 0) {
      return res.status(500).json({ error: 'No wish list generated' })
    }

    if (firstChunkSent) {
      res.write(JSON.stringify({ 
        chunkIndex: 1, 
        text: listText.trim(),
        isFirst: false,
        isComplete: true 
      }) + '\n')
    } else {
      if (listText.length >= 100) {
        const sentenceEnd = listText.search(/[.!?]\s+/)
        const firstChunk = sentenceEnd > 0 && sentenceEnd < listText.length * 0.6 
          ? listText.substring(0, sentenceEnd + 1).trim()
          : listText.substring(0, Math.min(200, listText.length)).trim()
        
        if (firstChunk.length >= 80) {
          res.write(JSON.stringify({ 
            chunkIndex: 0, 
            text: firstChunk, 
            isFirst: true,
            isComplete: false 
          }) + '\n')
          res.write(JSON.stringify({ 
            chunkIndex: 1, 
            text: listText.trim(),
            isFirst: false,
            isComplete: true 
          }) + '\n')
        } else {
          res.write(JSON.stringify({ 
            chunkIndex: 0, 
            text: listText.trim(), 
            isFirst: true,
            isComplete: true 
          }) + '\n')
        }
      } else {
        res.write(JSON.stringify({ 
          chunkIndex: 0, 
          text: listText.trim(), 
          isFirst: true,
          isComplete: true 
        }) + '\n')
      }
    }
    res.end()
  } catch (error) {
    console.error('âŒ Error generating wish list:', error)
    
    let statusCode = 500
    let errorMessage = 'Failed to generate wish list'

    if (error.message?.includes('Invalid authorization') || 
        error.message?.includes('authorization credentials') ||
        error.message?.includes('API key') || 
        error.message?.includes('authentication') ||
        error.message?.includes('401')) {
      statusCode = 401
      if (req.body?.apiKey) {
        errorMessage = 'Invalid authorization credentials. Please double-check your Inworld API Key.'
      } else {
        errorMessage = 'Invalid API key. Please check your GOOGLE_API_KEY and INWORLD_API_KEY in the .env file.'
      }
    } else if (error.message?.includes('rate limit') || error.message?.includes('quota')) {
      statusCode = 429
      errorMessage = 'Rate limit exceeded. Please wait a moment and try again.'
    } else if (error.message) {
      errorMessage = error.message
    }

    res.status(statusCode).json({ error: errorMessage })
  }
})

// New endpoint: Generate story with audio using Inworld Runtime
app.post('/api/generate-story-audio', async (req, res) => {
  console.log('\n\nðŸŽµ ==========================================')
  console.log('ðŸŽµ STORY + AUDIO GENERATION ENDPOINT CALLED (Inworld Runtime)')
  console.log('ðŸŽµ ==========================================')
  
  try {
    const { storyType, childName, voiceId } = req.body

    if (!storyType || !childName) {
      return res.status(400).json({ 
        error: 'Missing required fields: storyType and childName' 
      })
    }

    if (!process.env.GOOGLE_API_KEY) {
      return res.status(500).json({ 
        error: 'Server configuration error: GOOGLE_API_KEY not set' 
      })
    }

    if (!process.env.INWORLD_API_KEY) {
      return res.status(500).json({ 
        error: 'Server configuration error: INWORLD_API_KEY not set' 
      })
    }

    // Use voice ID from request, env variable, or default
    const selectedVoiceId = voiceId || process.env.INWORLD_VOICE_ID || 'Wendy'

    console.log(`ðŸŽµ Generating story with audio for "${childName}" about "${storyType}"`)
    console.log(`ðŸŽµ Voice ID - Request: "${voiceId}", Selected: "${selectedVoiceId}", Env: "${process.env.INWORLD_VOICE_ID}"`)

    // Create graph with API key and voice ID
    const graph = createGraph(process.env.INWORLD_API_KEY, selectedVoiceId)

    // Execute the Inworld Runtime graph
    const { outputStream } = await graph.start({
      childName,
      storyType,
    })

    // Collect audio chunks
    let storyText = ''

    // Collect all PCM samples first, then encode to WAV
    const SAMPLE_RATE = 24000
    let allFloat32Samples = []
    let firstChunkTime = null

    // Iterate through execution results using the proper pattern
    let done = false
    while (!done) {
      const result = await outputStream.next()
      
      await result.processResponse({
        ContentStream: async (contentStream) => {
          // Process LLM streaming content (collect text for logging)
          for await (const chunk of contentStream) {
            if (chunk.text) {
              storyText += chunk.text
            }
          }
        },
        TTSOutputStream: async (ttsStream) => {
          // TTS node outputs TTSOutputStream with audio.data (base64 encoded)
          for await (const chunk of ttsStream) {
            if (chunk.audio?.data) {
              // Audio data is base64-encoded PCM float32 samples
              const base64Data = chunk.audio.data
              const audioBuffer = Buffer.from(base64Data, 'base64')
              
              if (audioBuffer.length > 0) {
                if (firstChunkTime === null) {
                  firstChunkTime = Date.now()
                  console.log(`â±ï¸ First audio chunk received`)
                }
                
                // Convert buffer to Float32Array (4 bytes per float32 sample)
                const sampleCount = Math.floor(audioBuffer.length / 4)
                if (sampleCount > 0) {
                  // Create a properly aligned Float32Array
                  const float32Samples = new Float32Array(sampleCount)
                  const dataView = new DataView(audioBuffer.buffer, audioBuffer.byteOffset, sampleCount * 4)
                  for (let i = 0; i < sampleCount; i++) {
                    float32Samples[i] = dataView.getFloat32(i * 4, true) // little-endian
                  }
                  
                  // Collect samples
                  allFloat32Samples.push(float32Samples)
                }
              }
            }
          }
        },
        AudioStream: async (audioStream) => {
          // Fallback: handle AudioStream if TTSOutputStream doesn't work
          for await (const audioChunk of audioStream) {
            if (audioChunk.data) {
              const audioData = audioChunk.data
              let audioBuffer
              
              if (Array.isArray(audioData)) {
                audioBuffer = Buffer.from(audioData)
              } else if (typeof audioData === 'string') {
                // If it's base64 encoded
                audioBuffer = Buffer.from(audioData, 'base64')
              } else if (Buffer.isBuffer(audioData)) {
                audioBuffer = audioData
              } else {
                // Try to convert to buffer
                audioBuffer = Buffer.from(audioData)
              }
              
              if (audioBuffer && audioBuffer.length > 0) {
                res.write(audioBuffer)
                console.log(`ðŸŽµ Sent audio chunk (AudioStream): ${audioBuffer.length} bytes`)
              }
            }
          }
        },
        default: (data) => {
          // Handle other data types
          if (data?.text) {
            storyText += data.text
          }
        },
      })

      done = result.done
    }

    // Clean up
    await graph.stop()
    
    // Encode all collected samples to WAV
    if (allFloat32Samples.length > 0) {
      // Combine all Float32Arrays into one
      const totalSamples = allFloat32Samples.reduce((sum, arr) => sum + arr.length, 0)
      const combinedSamples = new Float32Array(totalSamples)
      let offset = 0
      for (const samples of allFloat32Samples) {
        combinedSamples.set(samples, offset)
        offset += samples.length
      }
      
      // Encode to WAV
      const wavData = {
        sampleRate: SAMPLE_RATE,
        channelData: [combinedSamples]
      }
      
      const wavBuffer = await wavEncoder.encode(wavData)
      
      if (!wavBuffer) {
        throw new Error('WAV encoding returned null or undefined')
      }
      
      // wav-encoder returns ArrayBuffer, convert to Buffer
      // ArrayBuffer has byteLength, not length
      const buffer = Buffer.from(wavBuffer)
      
      if (!buffer || buffer.length === 0) {
        throw new Error('WAV encoding returned empty buffer')
      }
      
      res.setHeader('Content-Type', 'audio/wav')
      res.setHeader('Content-Length', buffer.length.toString())
      res.write(buffer)
      console.log(`ðŸŽµ Sent WAV audio: ${buffer.length} bytes (${totalSamples} samples)`)
    }
    
    res.end()

    console.log(`âœ… Story generated: ${storyText.length} characters`)
  } catch (error) {
    console.error('âŒ Error generating story with audio:', error)
    
    if (!res.headersSent) {
      res.status(500).json({ error: error.message || 'Failed to generate story with audio' })
    }
  }
})

// Share story endpoint
app.post('/api/share-story', async (req, res) => {
  try {
    const { storyText, childName, voiceId, storyType, imageUrl, customApiKey, customVoiceId } = req.body

    if (!storyText) {
      return res.status(400).json({ error: 'Missing required field: storyText' })
    }

    // Generate a unique ID for this story
    const storyId = `story_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`

    // Store the story in memory
    sharedStories.set(storyId, {
      storyText,
      childName,
      voiceId,
      storyType,
      imageUrl: imageUrl || null,
      customApiKey,
      customVoiceId,
      createdAt: new Date().toISOString()
    })

    // Return just the storyId - frontend will construct the full URL from window.location.origin
    // This is the most stable approach as it doesn't require environment variables
    // and automatically works with any frontend URL (Vercel, localhost, etc.)
    res.json({ storyId })
  } catch (error) {
    console.error('Error sharing story:', error)
    res.status(500).json({ error: 'Failed to share story' })
  }
})

// Get shared story endpoint
app.get('/api/story/:id', async (req, res) => {
  try {
    const { id } = req.params
    const story = sharedStories.get(id)

    if (!story) {
      return res.status(404).json({ error: 'Story not found' })
    }

    res.json(story)
  } catch (error) {
    console.error('Error retrieving story:', error)
    res.status(500).json({ error: 'Failed to retrieve story' })
  }
})

app.listen(PORT, () => {
  console.log(`ðŸš€ Server running on port ${PORT}`)
  console.log(`ðŸ“– Story generation endpoint: http://localhost:${PORT}/api/generate-story`)
  console.log(`ðŸŽµ TTS endpoint: http://localhost:${PORT}/api/tts`)
  console.log(`ðŸŽ¤ Voice clone endpoint: http://localhost:${PORT}/api/clone-voice`)
})
